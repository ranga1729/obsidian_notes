![[Pasted image 20241208220246.png]]
Transformers are a type of deep learning model introduced in the paper "Attention is All You Need" in 2017.

#### Key features,
1. **Attention Mechanism**
The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding a particular word. This helps capture long-range dependencies and contextual relationships more effectively than previous models like RNNs and LSTMs.
2. **Encoder-Decoder Architecture**
![[Pasted image 20241208220608.png]]
The original transformer model has an encoder-decoder structure. The encoder processes the input sequence and generates a set of continuous representations, while the decoder generates the output sequence, using the encoder's output and previously generated tokens.
3. **Parallelization**
Unlike RNNs that process tokens sequentially, transformers process the entire input sequence at once. This allows for greater parallelization during training, significantly speeding up the process.
#### Component of a Transformer Model
1. **Self-Attention Layer**
This layer calculates attention scores for each token with respect to all other tokens in the sequence. These scores are used to create weighted representations of the tokens.
2. **Feed-Forward Neural Networks**
After the self-attention layer, the output is passed through a feed-forward neural network, which is applied to each token independently.
3. **Positional Encoding**
Since transformers do not process sequences sequentially, positional encodings are added to the input embeddings to provide information about the token positions in the sequence.
4. **Layer Normalization and Residual Connection**
Each sub-layer (self-attention and feed-forward) is followed by layer normalization and residual connections, helping to stabilize and accelerate the training process.
#### Applications
1. Text classification
2. Machine translation
3. Text generation
4. Named entity Recognition(NER)
5. Question Answering

#### Popular Transformer models
1. BERT(Bidirectional Encoder Representations from Transformers)
	Pre-trained on a large corpus of text, fine-tuned for various downstream tasks.
2. **GPT (Generative Pre-trained Transformer)** 
	Focuses on text generation and language modeling.
3. **T5 (Text-To-Text Transfer Transformer)**
	Converts all NLP tasks into a text-to-text format.
