Transformer models can be categorized based on their architecture and the tasks they are designed to perform,
1. Encoder-Only Transformers
2. Decoder-Only Transformers
3. Encoder-Decoder Transformers
4. Multimodal Transformers
5. Hybrid Transformers
6. Specialized Transformers
#### 1. Encoder-Only Transformers
Use only the encoder part of the transformer architecture.
Typically used for tasks that require understanding and encoding input sequences.
Ex: **BERT**(Bidirectional Encoder Representations from Transformers)
	Designed for tasks such as text classification, named entity recognition, and question answering.
	It processes text bidirectionally, meaning it looks at the entire context of a word (both left and right).
#### 2. Decoder-Only Transformers
Use only the decoder part of the transformer architecture.
Primarily used for tasks that involve generating sequences, such as text generation.
Ex: **GPT**(Generative Pre-trained Transformer)
	Designed for text generation tasks. 
	It processes text uni-directionally(Generates text from left to right).